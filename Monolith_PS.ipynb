{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Small attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zk4tOx0g18MM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "dense_params = np.random.rand(5)  # dense parameters\n",
        "sparse_params = {}  # sparse parameters stored as a dict\n",
        "\n",
        "def update_parameters(worker_dense_update, worker_sparse_update):\n",
        "    global dense_params\n",
        "    global sparse_params\n",
        "\n",
        "    # Updating dense parameters\n",
        "    dense_params = dense_params + worker_dense_update\n",
        "\n",
        "    # Updating sparse parameters\n",
        "    for id, embedding in worker_sparse_update.items():\n",
        "        if id in sparse_params:\n",
        "            sparse_params[id] = sparse_params[id] + embedding\n",
        "        else:\n",
        "            sparse_params[id] = embedding\n",
        "\n",
        "def train_step():\n",
        "    time.sleep(10)\n",
        "    # Small dense updates\n",
        "    worker_dense_update = np.random.rand(5) * 0.01  \n",
        "    worker_sparse_update = {random.randint(1, 100): np.random.rand(3) * 0.01 for _ in range(2)}\n",
        "    update_parameters(worker_dense_update, worker_sparse_update)\n",
        "\n",
        "def sync_parameters():\n",
        "    print(\"Synchronizing parameters...\")\n",
        "    print(\"Dense parameters:\", dense_params)\n",
        "    print(\"Sparse parameters:\")\n",
        "    for key, value in sparse_params.items():\n",
        "        print(f\"  ID {key}: {value}\")\n",
        "\n",
        "def expire_old_sparse_parameters(threshold_minutes=1):\n",
        "    current_time = datetime.now()\n",
        "    new_sparse_params = {}\n",
        "    for id, (embedding, timestamp) in sparse_params.items():\n",
        "        if (current_time - timestamp) < timedelta(minutes=threshold_minutes):\n",
        "            new_sparse_params[id] = (embedding, timestamp)\n",
        "    return new_sparse_params\n",
        "\n",
        "\n",
        "for _ in range(10):\n",
        "    train_step()\n",
        "\n",
        "sparse_params = {id: (embedding, datetime.now()) for id, embedding in sparse_params.items()}\n",
        "sparse_params = expire_old_sparse_parameters()\n",
        "\n",
        "sync_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPT response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUqwa_sXa2PG",
        "outputId": "769ac9d0-f235-449a-c2e2-aba32146b1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dense parameters in training PS: [-0.34595181  1.1959901  -0.93633135 -0.51692196  0.01520432]\n",
            "Final dense parameters in serving PS: [-0.34595181  1.1959901  -0.93633135 -0.51692196  0.01520432]\n",
            "Final sparse parameters for item5 in training PS: None\n",
            "Final sparse parameters for item5 in serving PS: None\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import hashlib\n",
        "import time\n",
        "\n",
        "# --------------------- Simulating Dense Parameters ----------------------\n",
        "# Dense parameters can be represented as a vector or a matrix (numpy array in this case)\n",
        "class DenseParameters:\n",
        "    def __init__(self, num_params):\n",
        "        self.params = np.random.randn(num_params)\n",
        "\n",
        "    def update(self, gradients):\n",
        "        learning_rate = 0.1\n",
        "        self.params -= learning_rate * gradients\n",
        "\n",
        "    def get_params(self):\n",
        "        return self.params\n",
        "\n",
        "\n",
        "# --------------------- Simulating Sparse Parameters ---------------------\n",
        "# Custom HashTable to simulate sparse parameter storage using Cuckoo hashing as described\n",
        "class CuckooHashTable:\n",
        "    def __init__(self):\n",
        "        self.table0 = {}\n",
        "        self.table1 = {}\n",
        "\n",
        "    def _hash0(self, key):\n",
        "        return int(hashlib.md5(key.encode('utf-8')).hexdigest(), 16) % 10  # Simplified hash function\n",
        "\n",
        "    def _hash1(self, key):\n",
        "        return int(hashlib.sha1(key.encode('utf-8')).hexdigest(), 16) % 10  # Simplified hash function\n",
        "\n",
        "    def insert(self, key, value):\n",
        "        index0 = self._hash0(key)\n",
        "        if key not in self.table0:\n",
        "            self.table0[index0] = (key, value)\n",
        "            return True\n",
        "        index1 = self._hash1(key)\n",
        "        if key not in self.table1:\n",
        "            self.table1[index1] = (key, value)\n",
        "            return True\n",
        "        return False  # For simplicity, we donâ€™t handle collisions or reinsertion\n",
        "\n",
        "    def lookup(self, key):\n",
        "        index0 = self._hash0(key)\n",
        "        if key in self.table0 and self.table0[index0][0] == key:\n",
        "            return self.table0[index0][1]\n",
        "        index1 = self._hash1(key)\n",
        "        if key in self.table1 and self.table1[index1][0] == key:\n",
        "            return self.table1[index1][1]\n",
        "        return None\n",
        "\n",
        "    def update(self, key, new_value):\n",
        "        index0 = self._hash0(key)\n",
        "        if key in self.table0 and self.table0[index0][0] == key:\n",
        "            self.table0[index0] = (key, new_value)\n",
        "            return True\n",
        "        index1 = self._hash1(key)\n",
        "        if key in self.table1 and self.table1[index1][0] == key:\n",
        "            self.table1[index1] = (key, new_value)\n",
        "            return True\n",
        "        return False  # For simplicity, we donâ€™t handle updating an item that doesn't exist\n",
        "\n",
        "\n",
        "# -------------------- Simulating Training and Synchronization Process -------------------\n",
        "\n",
        "# Create a model container with dense and sparse parameters\n",
        "class MonolithModel:\n",
        "    def __init__(self):\n",
        "        self.dense_params = DenseParameters(5)  # Fixed size dense parameters\n",
        "        self.sparse_params = CuckooHashTable()  # Dynamic sparse parameters represented by hash table\n",
        "\n",
        "    def train_dense(self, gradients):\n",
        "        self.dense_params.update(gradients)\n",
        "\n",
        "    def train_sparse(self, key, value):\n",
        "        self.sparse_params.insert(key, value)\n",
        "\n",
        "    def get_dense_params(self):\n",
        "        return self.dense_params.get_params()\n",
        "\n",
        "    def get_sparse_params(self, key):\n",
        "        return self.sparse_params.lookup(key)\n",
        "\n",
        "# Simulate worker and PS (parameter server) interactions\n",
        "def train_and_sync_model():\n",
        "    model_in_training_PS = MonolithModel()\n",
        "    model_in_serving_PS = MonolithModel()\n",
        "\n",
        "    # Training Loop, for simplicity we simulate both dense and sparse updates\n",
        "    for _ in range(10):  # Simulate 10 training steps\n",
        "        # Simulate sparse parameter training\n",
        "        example_id = \"item\"+str(random.randint(0, 9))   # Example sparse feature (item ID)\n",
        "        embedding_update = np.random.randn(5)            # Random embedding representing training update\n",
        "        model_in_training_PS.train_sparse(example_id, embedding_update)\n",
        "\n",
        "        # Simulate dense parameter training\n",
        "        dense_gradients = np.random.randn(5)             # Random gradient update\n",
        "        model_in_training_PS.train_dense(dense_gradients)\n",
        "\n",
        "        # Simulate synchronization from training_PS to serving_PS (dense parameters less frequently)\n",
        "        if _ % 2 == 0:  # Sync dense every 2 steps\n",
        "            model_in_serving_PS.dense_params.params = model_in_training_PS.get_dense_params()\n",
        "        # Sparse parameters can be synchronized more frequently\n",
        "        model_in_serving_PS.sparse_params.insert(example_id, model_in_training_PS.get_sparse_params(example_id))\n",
        "\n",
        "        time.sleep(1)  # Simulate time required for training step\n",
        "\n",
        "    # Print out the final parameters for comparison\n",
        "    print(\"Final dense parameters in training PS:\", model_in_training_PS.get_dense_params())\n",
        "    print(\"Final dense parameters in serving PS:\", model_in_serving_PS.get_dense_params())\n",
        "\n",
        "    # Print out an example of final sparse parameters for a comparison\n",
        "    example_id = \"item5\"\n",
        "    print(\"Final sparse parameters for item5 in training PS:\", model_in_training_PS.get_sparse_params(example_id))\n",
        "    print(\"Final sparse parameters for item5 in serving PS:\", model_in_serving_PS.get_sparse_params(example_id))\n",
        "\n",
        "train_and_sync_model()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
